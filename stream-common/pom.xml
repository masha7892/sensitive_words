<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.example</groupId>
    <artifactId>stream-common</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <flink.version>1.17.1</flink.version>
        <hadoop.version>3.0.0</hadoop.version>
    </properties>

    <dependencies>

<!--        slf4j-log4j12 只是一个 binding（桥接器），负责将 SLF4J 日志请求转发给 Log4j 1.2-->
<!--        但它并不包含 SLF4J 的接口定义（即 slf4j-api）-->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
            <version>1.7.7</version>
<!--            <scope>provided</scope>-->
        </dependency>

<!--        使用 slf4j 日志门面的基础依赖，绝大多数使用 slf4j 规范的项目都需要引入-->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>1.7.25</version>
            <!--                <scope>provided</scope>-->
        </dependency>


        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
            <version>30.0-android</version>
        </dependency>



        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>1.18.20</version>
        </dependency>


        <!-- flink 相关依赖 开始 -->

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-hbase_2.11</artifactId>
            <version>1.10.3</version>
            <!--                <scope>provided</scope>-->
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>2.4.7</version>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-orc</artifactId>
            <version>${flink.version}</version>
        </dependency>


        <!--        flink 流批开发依赖包-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <!--        flink 本地WebUI 依赖-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-runtime-web</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>

        <!--        kafka 依赖-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <!--        flink RocksDB 依赖-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-statebackend-rocksdb</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <!--        flink table 依赖环境-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-runtime</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-loader</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-files</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <!--        Table API 依赖-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <!--        Table API + DataStream 相互集成 -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <!--        Flink 解析JSON-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-json</artifactId>
            <version>${flink.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

        <!--        Flink CDC MySQL connector-->
        <dependency>
            <groupId>com.ververica</groupId>
            <artifactId>flink-connector-mysql-cdc</artifactId>
            <version>2.4.0</version>
            <!--                <scope>provided</scope>-->
        </dependency>




        <dependency>
            <groupId>com.alibaba.hologres</groupId>
            <artifactId>hologres-connector-flink-1.17</artifactId>
            <version>1.4.2</version>
            <classifier>jar-with-dependencies</classifier>
            <scope>provided</scope>
        </dependency>

        <!-- flink 相关依赖 结束 -->



        <!-- hadoop 相关依赖 开发 -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
        </dependency>

        <!--如果保存检查点到hdfs上，需要引入此依赖-->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
            <!--                <scope>provided</scope>-->
        </dependency>

<!--        <dependency>-->
<!--            <groupId>org.apache.hadoop</groupId>-->
<!--            <artifactId>hadoop-auth</artifactId>-->
<!--            &lt;!&ndash;                <scope>provided</scope>&ndash;&gt;-->
<!--            <exclusions>-->
<!--                <exclusion>-->
<!--                    <groupId>org.slf4j</groupId>-->
<!--                    <artifactId>slf4j-reload4j</artifactId>-->
<!--                </exclusion>-->
<!--            </exclusions>-->
<!--        </dependency>-->
        <!-- hadoop 相关依赖 结束 -->

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>2.0.3</version>
        </dependency>

    </dependencies>









<!--配置文件
定义了不同环境的配置
dev：开发环境（默认激活
stage：测试环境
prod：生产环境

每个环境使用不同的配置文件：
common-config.properties.dev
common-config.properties.stage
common-config.properties.prod

切换打包时的环境
1. 右键点击 package
2. 选择 "编辑运行配置"
3. 在 "配置文件" 栏中：
   - 只勾选 "dev"
   - 取消其他所有选项
4. 点击 "确定"
5. 点击 "运行" 按钮

其他选项
1. dev、stage、prod：这些是您在 pom.xml 中定义的环境配置
2. -dev、-stage、-prod：这些是排除选项，不要选择
3. custom-repositories：自定义 Maven 仓库配置
4. jdk-1.8：JDK 版本配置
-->
    <profiles>
        <profile>
            <id>dev</id>
            <activation>
                <activeByDefault>true</activeByDefault>
            </activation>
            <properties>
<!--
<program.scope>provided</program.scope>
的配置主要是为了控制依赖的打包行为，确保在特定环境下使用容器提供的依赖，而不是打包到应用中。

Maven 依赖作用域的类型：
    1. compile：默认值，编译和运行时都需要
    2. provided：编译时需要，运行时由容器提供
    3. runtime：运行时需要，编译时不需要
    4. test：测试时需要，编译和运行时不需要
    5. system：类似 provided，但需要显式指定 jar 包路径
    6. import：用于依赖管理，不会实际引入依赖

provided 的具体含义：
    编译时需要这个依赖
    运行时由容器（如 Tomcat、JBoss 等）提供
    打包时不会包含这个依赖
    常用于避免依赖冲突


与 compile 的区别：
    compile：依赖会被打包到最终的 jar/war 中
    provided：依赖不会被打包，由容器提供
-->
                <program.scope>compile</program.scope>
            </properties>
            <build>
                <filters>
                    <filter>src/main/resources/filter/common-config.properties.dev</filter>
                </filters>
            </build>
        </profile>

        <profile>
            <id>stage</id>
            <properties>
                <program.scope>provided</program.scope>
            </properties>
            <build>
                <filters>
                    <filter>src/main/resources/filter/common-config.properties.stage</filter>
                </filters>
            </build>
        </profile>

        <profile>
            <id>prod</id>
            <build>
                <filters>
                    <filter>src/main/resources/filter/common-config.properties.prod</filter>
                </filters>
            </build>
            <properties>
                <program.scope>provided</program.scope>
            </properties>
        </profile>
    </profiles>






    <build>

        <!--Build 插件配置-->
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-resources-plugin</artifactId>
                <version>3.3.1</version>
                <executions>
                    <execution>
                        <id>copy-conf</id>
 <!--default生命周期的哪个阶段-->
 <!--
 在 Apache Maven 的默认 Build（默认）生命周期 中，
 process-resources 是紧跟在 generate-resources 之后、compile 之前的一个阶段
 将项目中 src/main/resources下的资源文件复制到target/classes
 如果在 POM 中为该资源指定 <filtering>true</filtering>，Maven 会使用 <filters> 文件中的键值对替换资源文件中的 ${…} 占位符
 -->
                        <phase>process-resources</phase>
                        <goals>
                            <goal>resources</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
<!--
在 package 阶段打包创建一个包含所有依赖的 JAR 文件
生成 xxx-jar-with-dependencies.jar
-->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>2.4</version>
                <configuration>
<!-- 设置false后是去掉 xxx-jar-with-dependencies.jar 后的 “-jar-with-dependencies” -->
<!--                    <appendAssemblyId>false</appendAssemblyId>-->
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>assembly</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>



<!--build的Resources 配置  负责处理项目资源文件并拷贝到输出目录-->
<!--
定义了资源文件的处理规则：
    处理 src/main/resources 目录下的文件
    包含 .conf、.csv、.properties、.xml 文件
    排除 filter 目录下的文件
    启用变量替换（filtering=true）
-->
        <resources>
            <resource>
                <directory>src/main/resources/</directory>
                <targetPath>./</targetPath>
                <filtering>true</filtering>
                <includes>
                    <include>**/*.conf</include>
                    <include>**/*.csv</include>
                    <include>**/*.properties</include>
                    <include>**/*.xml</include>
                </includes>
                <!-- 排除标签 -->
                <excludes>
                    <exclude>filter/*</exclude>
                </excludes>
            </resource>
        </resources>
    </build>

</project>